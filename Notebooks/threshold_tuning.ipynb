{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a495406",
   "metadata": {},
   "source": [
    "## CELL 1: Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "4c0b3359",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports successful!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from XAI_engine.xai_engine import FakeReviewXAI\n",
    "\n",
    "print(\"Imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04b4b48",
   "metadata": {},
   "source": [
    "## CELL 2: Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "70340995",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reviews: 40,432\n",
      "\n",
      "Label distribution:\n",
      "label\n",
      "CG    20216\n",
      "OR    20216\n",
      "Name: count, dtype: int64\n",
      "\n",
      "CG = Computer Generated (FAKE)\n",
      "OR = Original (GENUINE)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../data/fake_reviews_dataset.csv')\n",
    "\n",
    "print(f\"Total reviews: {len(df):,}\")\n",
    "print(f\"\\nLabel distribution:\")\n",
    "print(df['label'].value_counts())\n",
    "print(f\"\\nCG = Computer Generated (FAKE)\")\n",
    "print(f\"OR = Original (GENUINE)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbebcabe",
   "metadata": {},
   "source": [
    "## CELL 3: Prepare Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "b0c75183",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set: 200 reviews\n",
      "Fake: 100\n",
      "Genuine: 100\n"
     ]
    }
   ],
   "source": [
    "n_samples = 100\n",
    "\n",
    "fake_samples = df[df['label'] == 'CG'].sample(n=n_samples, random_state=42)\n",
    "genuine_samples = df[df['label'] == 'OR'].sample(n=n_samples, random_state=42)\n",
    "\n",
    "validation_df = pd.concat([fake_samples, genuine_samples])\n",
    "validation_df['is_fake'] = validation_df['label'] == 'CG'\n",
    "validation_df = validation_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(f\"Validation set: {len(validation_df)} reviews\")\n",
    "print(f\"Fake: {validation_df['is_fake'].sum()}\")\n",
    "print(f\"Genuine: {(~validation_df['is_fake']).sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ca8962",
   "metadata": {},
   "source": [
    "## CELL 4: Tuning Function\n",
    "Purpose: Test XAI engine on reviews and record predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "01c7bb02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning function ready\n"
     ]
    }
   ],
   "source": [
    "def tune_thresholds(reviews_df, xai_engine):\n",
    "    \n",
    "    \"\"\"Test XAI engine on labeled data\"\"\"\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for idx, row in reviews_df.iterrows():\n",
    "        try:\n",
    "            analysis = xai_engine.analyze_review(row['text_'])\n",
    "            \n",
    "            predicted_fake = analysis['verdict'] in ['LIKELY FAKE', 'SUSPICIOUS']\n",
    "            actual_fake = row['is_fake']\n",
    "            \n",
    "            results.append({\n",
    "                'actual_fake': actual_fake,\n",
    "                'predicted_fake': predicted_fake,\n",
    "                'verdict': analysis['verdict'],\n",
    "                'confidence': analysis['confidence'],\n",
    "                'flag_count': analysis['flag_count'],\n",
    "                'correct': predicted_fake == actual_fake\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"Error on review {idx}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"Tuning function ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f8455e",
   "metadata": {},
   "source": [
    "## CELL 5: Calculate Metrics function\n",
    "Purpose: Compute accuracy, precision, recall, F1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "57cf2301",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics function ready\n"
     ]
    }
   ],
   "source": [
    "def calculate_metrics(results):\n",
    "    \n",
    "    \"\"\"Calculate performance metrics\"\"\"\n",
    "    \n",
    "    tp = sum(1 for r in results if r['actual_fake'] and r['predicted_fake'])\n",
    "    fp = sum(1 for r in results if not r['actual_fake'] and r['predicted_fake'])\n",
    "    tn = sum(1 for r in results if not r['actual_fake'] and not r['predicted_fake'])\n",
    "    fn = sum(1 for r in results if r['actual_fake'] and not r['predicted_fake'])\n",
    "    \n",
    "    total = len(results)\n",
    "    accuracy = (tp + tn) / total if total > 0 else 0\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1 = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1,\n",
    "        'true_positives': tp,\n",
    "        'false_positives': fp,\n",
    "        'true_negatives': tn,\n",
    "        'false_negatives': fn,\n",
    "        'total_samples': total\n",
    "    }\n",
    "\n",
    "print(\"Metrics function ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef8db1a",
   "metadata": {},
   "source": [
    "## Cell 6 : define Threshold Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "4c3aaa8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing 3 data-tuned configurations:\n",
      "  - Data-Tuned Conservative\n",
      "  - Data-Tuned Baseline\n",
      "  - Data-Tuned Relaxed\n"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "# CELL 6: Define Threshold Configurations\n",
    "# ========================================\n",
    "# Based on feature analysis: Fake vs Real averages\n",
    "# Fake: sentiment=0.624, word_count=55.75, adj_noun=0.649, first_person=0.051, uniqueness=0.767\n",
    "# Real: sentiment=0.559, word_count=66.73, adj_noun=0.510, first_person=0.045, uniqueness=0.839\n",
    "\n",
    "threshold_configs = [\n",
    "    {\n",
    "        'name': 'Data-Tuned Conservative',\n",
    "        'SENTIMENT_EXTREME': 0.65,  # Above real avg (0.559), catches strong fake sentiment\n",
    "        'WORD_COUNT_MIN': 58,      # Below fake avg (55.75), flags short fakes\n",
    "        'WORD_COUNT_MAX': 200,\n",
    "        'ADJ_NOUN_RATIO': 0.62,    # Below fake avg (0.649), catches descriptive fakes\n",
    "        'FIRST_PERSON_RATIO': 0.048,  # Below fake avg (0.051), flags self-referencing fakes\n",
    "        'CAPS_RATIO_MAX': 0.01,    # Above fake avg (0.002), flags caps in reals\n",
    "        'EXCESSIVE_PUNCT_MIN': 1,  # Low, since counts are small\n",
    "        'UNIQUENESS_RATIO_MIN': 0.79,  # Above fake avg (0.767), flags repetitive fakes\n",
    "        # Tier 3: Advanced\n",
    "        'FLESCH_MAX': 85,          # Stricter readability threshold\n",
    "        'DALE_CHALL_MAX': 7.0,     # Stricter vocabulary threshold\n",
    "        'BIGRAM_REPETITIVENESS_MIN': 0.90,  # Stricter repetition threshold\n",
    "        'COMMON_FAKE_NGRAMS_MIN': 3,  # Stricter n-gram threshold\n",
    "        'TFIDF_SIMILARITY_MAX': 0.4  # Stricter similarity threshold\n",
    "    },\n",
    "    {\n",
    "        'name': 'Data-Tuned Baseline',\n",
    "        'SENTIMENT_EXTREME': 0.60,  # Midpoint, balances fake/real\n",
    "        'WORD_COUNT_MIN': 60,      # Between fake (55.75) and real (66.73)\n",
    "        'WORD_COUNT_MAX': 200,\n",
    "        'ADJ_NOUN_RATIO': 0.58,    # Midpoint between 0.510 and 0.649\n",
    "        'FIRST_PERSON_RATIO': 0.048,\n",
    "        'CAPS_RATIO_MAX': 0.01,\n",
    "        'EXCESSIVE_PUNCT_MIN': 1,\n",
    "        'UNIQUENESS_RATIO_MIN': 0.80,  # Closer to real avg (0.839)\n",
    "        # Tier 3: Advanced\n",
    "        'FLESCH_MAX': 90,\n",
    "        'DALE_CHALL_MAX': 8.0,\n",
    "        'BIGRAM_REPETITIVENESS_MIN': 0.95,\n",
    "        'COMMON_FAKE_NGRAMS_MIN': 2,\n",
    "        'TFIDF_SIMILARITY_MAX': 0.5\n",
    "    },\n",
    "    {\n",
    "        'name': 'Data-Tuned Relaxed',\n",
    "        'SENTIMENT_EXTREME': 0.55,  # Below real avg, more flags\n",
    "        'WORD_COUNT_MIN': 62,      # Higher, fewer short flags\n",
    "        'WORD_COUNT_MAX': 200,\n",
    "        'ADJ_NOUN_RATIO': 0.54,    # Lower, more descriptive flags\n",
    "        'FIRST_PERSON_RATIO': 0.046,  # Lower, more self-ref flags\n",
    "        'CAPS_RATIO_MAX': 0.005,   # Lower, more caps flags\n",
    "        'EXCESSIVE_PUNCT_MIN': 0,  # Very relaxed\n",
    "        'UNIQUENESS_RATIO_MIN': 0.82,  # Higher, fewer redundancy flags\n",
    "        # Tier 3: Advanced\n",
    "        'FLESCH_MAX': 95,          # More relaxed readability\n",
    "        'DALE_CHALL_MAX': 9.0,     # More relaxed vocabulary\n",
    "        'BIGRAM_REPETITIVENESS_MIN': 0.98,  # More relaxed repetition\n",
    "        'COMMON_FAKE_NGRAMS_MIN': 1,  # More relaxed n-gram\n",
    "        'TFIDF_SIMILARITY_MAX': 0.6  # More relaxed similarity\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"Testing 3 data-tuned configurations:\")\n",
    "for config in threshold_configs:\n",
    "    print(f\"  - {config['name']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc708b4",
   "metadata": {},
   "source": [
    "## Cell 7: Test For the Best configuration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "42f1f4a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Testing: Data-Tuned Conservative\n",
      "============================================================\n",
      "Accuracy:  50.50%\n",
      "Precision: 50.28%\n",
      "Recall:    91.00%\n",
      "F1 Score:  64.77%\n",
      "TP: 91, FP: 90\n",
      "TN: 10, FN: 9\n",
      "\n",
      "============================================================\n",
      "Testing: Data-Tuned Baseline\n",
      "============================================================\n",
      "Accuracy:  50.00%\n",
      "Precision: 50.00%\n",
      "Recall:    93.00%\n",
      "F1 Score:  65.03%\n",
      "TP: 93, FP: 93\n",
      "TN: 7, FN: 7\n",
      "\n",
      "============================================================\n",
      "Testing: Data-Tuned Relaxed\n",
      "============================================================\n",
      "Accuracy:  50.50%\n",
      "Precision: 50.25%\n",
      "Recall:    100.00%\n",
      "F1 Score:  66.89%\n",
      "TP: 100, FP: 99\n",
      "TN: 1, FN: 0\n",
      "\n",
      "All configurations tested!\n"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "# CELL 7: Test All Configurations\n",
    "# ========================================\n",
    "all_results = []\n",
    "\n",
    "for config in threshold_configs:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Testing: {config['name']}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Create XAI engine with these thresholds\n",
    "    xai = FakeReviewXAI()\n",
    "    \n",
    "    # Update thresholds (copy config, remove 'name')\n",
    "    thresholds = {k: v for k, v in config.items() if k != 'name'}\n",
    "    xai.thresholds.update(thresholds)\n",
    "    \n",
    "    # Run tuning\n",
    "    results = tune_thresholds(validation_df, xai)\n",
    "    metrics = calculate_metrics(results)\n",
    "    \n",
    "    # Store\n",
    "    config_result = config.copy()\n",
    "    config_result.update(metrics)\n",
    "    all_results.append(config_result)\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"Accuracy:  {metrics['accuracy']:.2%}\")\n",
    "    print(f\"Precision: {metrics['precision']:.2%}\")\n",
    "    print(f\"Recall:    {metrics['recall']:.2%}\")\n",
    "    print(f\"F1 Score:  {metrics['f1_score']:.2%}\")\n",
    "    print(f\"TP: {metrics['true_positives']}, FP: {metrics['false_positives']}\")\n",
    "    print(f\"TN: {metrics['true_negatives']}, FN: {metrics['false_negatives']}\")\n",
    "\n",
    "print(\"\\nAll configurations tested!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cebfb59",
   "metadata": {},
   "source": [
    "## CELL 8: Save Results to threshold_tuning.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "e4dc282f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to: data/threshold_tuning.csv\n",
      "\n",
      "Results:\n",
      "                      name  accuracy  precision  recall  f1_score\n",
      "0  Data-Tuned Conservative     0.505   0.502762    0.91  0.647687\n",
      "1      Data-Tuned Baseline     0.500   0.500000    0.93  0.650350\n",
      "2       Data-Tuned Relaxed     0.505   0.502513    1.00  0.668896\n"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "# CELL 8: Save Results to CSV\n",
    "# ========================================\n",
    "results_df = pd.DataFrame(all_results)\n",
    "\n",
    "column_order = [\n",
    "    'name', 'SENTIMENT_EXTREME', 'WORD_COUNT_MIN', 'WORD_COUNT_MAX',\n",
    "    'ADJ_NOUN_RATIO', 'FIRST_PERSON_RATIO',\n",
    "    'CAPS_RATIO_MAX', 'EXCESSIVE_PUNCT_MIN', 'UNIQUENESS_RATIO_MIN',\n",
    "    'FLESCH_MAX', 'DALE_CHALL_MAX', 'BIGRAM_REPETITIVENESS_MIN',\n",
    "    'COMMON_FAKE_NGRAMS_MIN', 'TFIDF_SIMILARITY_MAX',\n",
    "    'accuracy', 'precision', 'recall', 'f1_score',\n",
    "    'true_positives', 'false_positives', 'true_negatives', 'false_negatives',\n",
    "    'total_samples'\n",
    "]\n",
    "\n",
    "results_df = results_df[column_order]\n",
    "results_df.to_csv('../data/threshold_tuning.csv', index=False)\n",
    "\n",
    "print(\"Results saved to: data/threshold_tuning.csv\")\n",
    "print(\"\\nResults:\")\n",
    "print(results_df[['name', 'accuracy', 'precision', 'recall', 'f1_score']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc56833e",
   "metadata": {},
   "source": [
    "## CELL 9: Display Best Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "2d71fe48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "BEST CONFIGURATION (By Accuracy)\n",
      "============================================================\n",
      "\n",
      "Name: Data-Tuned Conservative\n",
      "\n",
      "Performance:\n",
      "  Accuracy:  50.50%\n",
      "  Precision: 50.28%\n",
      "  Recall:    91.00%\n",
      "  F1 Score:  64.77%\n",
      "\n",
      "Thresholds:\n",
      "  SENTIMENT_EXTREME:      0.65\n",
      "  WORD_COUNT_MIN:         58\n",
      "  WORD_COUNT_MAX:         200\n",
      "  ADJ_NOUN_RATIO:         0.62\n",
      "  FIRST_PERSON_RATIO:     0.048\n",
      "  CAPS_RATIO_MAX:         0.01\n",
      "  EXCESSIVE_PUNCT_MIN:    1\n",
      "  UNIQUENESS_RATIO_MIN:   0.79\n",
      "  FLESCH_MAX:             85\n",
      "  DALE_CHALL_MAX:         7.0\n",
      "  BIGRAM_REPETITIVENESS_MIN: 0.9\n",
      "  COMMON_FAKE_NGRAMS_MIN: 3\n",
      "  TFIDF_SIMILARITY_MAX:   0.4\n"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "# CELL 9: Find Best Configuration\n",
    "# ========================================\n",
    "best_idx = results_df['accuracy'].idxmax()\n",
    "best_config = results_df.iloc[best_idx]\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"BEST CONFIGURATION (By Accuracy)\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nName: {best_config['name']}\")\n",
    "print(f\"\\nPerformance:\")\n",
    "print(f\"  Accuracy:  {best_config['accuracy']:.2%}\")\n",
    "print(f\"  Precision: {best_config['precision']:.2%}\")\n",
    "print(f\"  Recall:    {best_config['recall']:.2%}\")\n",
    "print(f\"  F1 Score:  {best_config['f1_score']:.2%}\")\n",
    "print(f\"\\nThresholds:\")\n",
    "print(f\"  SENTIMENT_EXTREME:      {best_config['SENTIMENT_EXTREME']}\")\n",
    "print(f\"  WORD_COUNT_MIN:         {best_config['WORD_COUNT_MIN']}\")\n",
    "print(f\"  WORD_COUNT_MAX:         {best_config['WORD_COUNT_MAX']}\")\n",
    "print(f\"  ADJ_NOUN_RATIO:         {best_config['ADJ_NOUN_RATIO']}\")\n",
    "\n",
    "print(f\"  FIRST_PERSON_RATIO:     {best_config['FIRST_PERSON_RATIO']}\")\n",
    "print(f\"  CAPS_RATIO_MAX:         {best_config['CAPS_RATIO_MAX']}\")\n",
    "print(f\"  EXCESSIVE_PUNCT_MIN:    {best_config['EXCESSIVE_PUNCT_MIN']}\")\n",
    "print(f\"  UNIQUENESS_RATIO_MIN:   {best_config['UNIQUENESS_RATIO_MIN']}\")\n",
    "\n",
    "print(f\"  FLESCH_MAX:             {best_config.get('FLESCH_MAX')}\")\n",
    "print(f\"  DALE_CHALL_MAX:         {best_config.get('DALE_CHALL_MAX')}\")\n",
    "print(f\"  BIGRAM_REPETITIVENESS_MIN: {best_config.get('BIGRAM_REPETITIVENESS_MIN')}\")\n",
    "print(f\"  COMMON_FAKE_NGRAMS_MIN: {best_config.get('COMMON_FAKE_NGRAMS_MIN')}\")\n",
    "print(f\"  TFIDF_SIMILARITY_MAX:   {best_config.get('TFIDF_SIMILARITY_MAX')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b3e15b",
   "metadata": {},
   "source": [
    "## CELL 11: Generate config.py Code\n",
    "updates config.py with optimal thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "2971eb59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config.py successfully updated!\n",
      "\n",
      "File location: c:\\Users\\Amr\\Documents\\GitHub\\Team-C-repository\\xai_engine\\config.py\n"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "# CELL 11: Auto-Update config.py\n",
    "# ========================================\n",
    "import os\n",
    "\n",
    "\n",
    "# Generate the new config content\n",
    "new_config_content = f'''\"\"\"\n",
    "XAI Engine Configuration\n",
    "Centralized thresholds for all features (Tier 1 + Tier 2 + Tier 3)\n",
    "Auto-generated by threshold_tuning.ipynb\n",
    "\"\"\"\n",
    "\n",
    "# ============================================================================\n",
    "# ALL THRESHOLDS (TIER 1 + TIER 2 + TIER 3) - OPTIMIZED\n",
    "# ============================================================================\n",
    "\n",
    "THRESHOLDS = {{\n",
    "    # TIER 1: Essential Features\n",
    "    'SENTIMENT_EXTREME': {best_config['SENTIMENT_EXTREME']},\n",
    "    'WORD_COUNT_MIN': {int(best_config['WORD_COUNT_MIN'])},\n",
    "    'WORD_COUNT_MAX': {int(best_config['WORD_COUNT_MAX'])},\n",
    "    'ADJ_NOUN_RATIO': {best_config['ADJ_NOUN_RATIO']},\n",
    "    'FIRST_PERSON_RATIO': {best_config['FIRST_PERSON_RATIO']},\n",
    "    \n",
    "    # TIER 2: Important Features\n",
    "    'CAPS_RATIO_MAX': {best_config['CAPS_RATIO_MAX']},\n",
    "    'EXCESSIVE_PUNCT_MIN': {int(best_config['EXCESSIVE_PUNCT_MIN'])},\n",
    "    'UNIQUENESS_RATIO_MIN': {best_config['UNIQUENESS_RATIO_MIN']},\n",
    "    \n",
    "    # TIER 3: Advanced Features\n",
    "    'FLESCH_MAX': {best_config['FLESCH_MAX']},  # Flag if readability too high (fake-like simplicity)\n",
    "    'DALE_CHALL_MAX':{best_config['DALE_CHALL_MAX']},  # Flag if too simple vocabulary\n",
    "    'BIGRAM_REPETITIVENESS_MIN': {best_config['BIGRAM_REPETITIVENESS_MIN']},  # Flag if too repetitive\n",
    "    'COMMON_FAKE_NGRAMS_MIN': {best_config['COMMON_FAKE_NGRAMS_MIN']},  # Flag if >=2 common fake phrases\n",
    "    'TFIDF_SIMILARITY_MAX': {best_config['TFIDF_SIMILARITY_MAX']}  # Flag if too similar to generic reviews\n",
    "}}\n",
    "\n",
    "\n",
    "def load_config():\n",
    "    \"\"\"Return a copy of thresholds\"\"\"\n",
    "    return THRESHOLDS.copy()\n",
    "\n",
    "\n",
    "def update_thresholds(new_thresholds):\n",
    "    \"\"\"Update thresholds (used by threshold tuning)\"\"\"\n",
    "    THRESHOLDS.update(new_thresholds)\n",
    "'''\n",
    "\n",
    "# Write to config.py\n",
    "config_path = '../xai_engine/config.py'\n",
    "\n",
    "try:\n",
    "    with open(config_path, 'w') as f:\n",
    "        f.write(new_config_content)\n",
    "    \n",
    "    print(\"config.py successfully updated!\")\n",
    "    print(f\"\\nFile location: {os.path.abspath(config_path)}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error updating config.py: {e}\")\n",
    "    print(\"\\nManual update required. Copy this code:\\n\")\n",
    "    print(new_config_content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
