{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a495406",
   "metadata": {},
   "source": [
    "## CELL 1: Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "4c0b3359",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports successful!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from XAI_engine.xai_engine import FakeReviewXAI\n",
    "\n",
    "print(\"Imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04b4b48",
   "metadata": {},
   "source": [
    "## CELL 2: Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "70340995",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reviews: 40,432\n",
      "\n",
      "Label distribution:\n",
      "label\n",
      "CG    20216\n",
      "OR    20216\n",
      "Name: count, dtype: int64\n",
      "\n",
      "CG = Computer Generated (FAKE)\n",
      "OR = Original (GENUINE)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../data/fake_reviews_dataset.csv')\n",
    "\n",
    "print(f\"Total reviews: {len(df):,}\")\n",
    "print(f\"\\nLabel distribution:\")\n",
    "print(df['label'].value_counts())\n",
    "print(f\"\\nCG = Computer Generated (FAKE)\")\n",
    "print(f\"OR = Original (GENUINE)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbebcabe",
   "metadata": {},
   "source": [
    "## CELL 3: Prepare Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "b0c75183",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set: 1000 reviews\n",
      "Fake: 500\n",
      "Genuine: 500\n"
     ]
    }
   ],
   "source": [
    "n_samples = 500\n",
    "\n",
    "fake_samples = df[df['label'] == 'CG'].sample(n=n_samples, random_state=42)\n",
    "genuine_samples = df[df['label'] == 'OR'].sample(n=n_samples, random_state=42)\n",
    "\n",
    "validation_df = pd.concat([fake_samples, genuine_samples])\n",
    "validation_df['is_fake'] = validation_df['label'] == 'CG'\n",
    "validation_df = validation_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(f\"Validation set: {len(validation_df)} reviews\")\n",
    "print(f\"Fake: {validation_df['is_fake'].sum()}\")\n",
    "print(f\"Genuine: {(~validation_df['is_fake']).sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ca8962",
   "metadata": {},
   "source": [
    "## CELL 4: Tuning Function\n",
    "Purpose: Test XAI engine on reviews and record predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "01c7bb02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning function ready\n"
     ]
    }
   ],
   "source": [
    "def tune_thresholds(reviews_df, xai_engine):\n",
    "    \n",
    "    \"\"\"Test XAI engine on labeled data\"\"\"\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for idx, row in reviews_df.iterrows():\n",
    "        try:\n",
    "            analysis = xai_engine.analyze_review(row['text_'])\n",
    "            \n",
    "            predicted_fake = analysis['verdict'] in ['LIKELY FAKE', 'SUSPICIOUS']\n",
    "            actual_fake = row['is_fake']\n",
    "            \n",
    "            results.append({\n",
    "                'actual_fake': actual_fake,\n",
    "                'predicted_fake': predicted_fake,\n",
    "                'verdict': analysis['verdict'],\n",
    "                'confidence': analysis['confidence'],\n",
    "                'flag_count': analysis['flag_count'],\n",
    "                'correct': predicted_fake == actual_fake\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"Error on review {idx}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"Tuning function ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f8455e",
   "metadata": {},
   "source": [
    "## CELL 5: Calculate Metrics function\n",
    "Purpose: Compute accuracy, precision, recall, F1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "57cf2301",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics function ready\n"
     ]
    }
   ],
   "source": [
    "def calculate_metrics(results):\n",
    "    \n",
    "    \"\"\"Calculate performance metrics\"\"\"\n",
    "    \n",
    "    tp = sum(1 for r in results if r['actual_fake'] and r['predicted_fake'])\n",
    "    fp = sum(1 for r in results if not r['actual_fake'] and r['predicted_fake'])\n",
    "    tn = sum(1 for r in results if not r['actual_fake'] and not r['predicted_fake'])\n",
    "    fn = sum(1 for r in results if r['actual_fake'] and not r['predicted_fake'])\n",
    "    \n",
    "    total = len(results)\n",
    "    accuracy = (tp + tn) / total if total > 0 else 0\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1 = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1,\n",
    "        'true_positives': tp,\n",
    "        'false_positives': fp,\n",
    "        'true_negatives': tn,\n",
    "        'false_negatives': fn,\n",
    "        'total_samples': total\n",
    "    }\n",
    "\n",
    "print(\"Metrics function ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef8db1a",
   "metadata": {},
   "source": [
    "## Cell 6 : define Threshold Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "4c3aaa8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing 3 data-tuned configurations:\n",
      "  - Conservative (High Precision)\n",
      "  - Balanced (Standard)\n",
      "  - Aggressive (High Recall)\n"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "# CELL 6: Define Threshold Configurations\n",
    "# ========================================\n",
    "# Based on feature analysis: Fake vs Real averages\n",
    "# Fake: sentiment=0.624, word_count=55.75, adj_noun=0.649, first_person=0.051, uniqueness=0.767\n",
    "# Real: sentiment=0.559, word_count=66.73, adj_noun=0.510, first_person=0.045, uniqueness=0.839\n",
    "\n",
    "threshold_configs = [\n",
    "    {\n",
    "        'name': 'Conservative (High Precision)',\n",
    "        # Tier 1: Essential Features\n",
    "        'SENTIMENT_EXTREME': 0.85,      # Only flag very extreme sentiment\n",
    "        'WORD_COUNT_MIN': 10,           # Only flag very short reviews\n",
    "        'WORD_COUNT_MAX': 250,          # Allow longer reviews\n",
    "        'ADJ_NOUN_RATIO': 3.0,          # Only flag heavily adjective-heavy text\n",
    "        'FIRST_PERSON_RATIO': 0.20,     # Only flag excessive self-reference\n",
    "        \n",
    "        # Tier 2: Important Features\n",
    "        'SPAM_KEYWORD_MIN': 4,          # Need many spam words to flag\n",
    "        'CAPS_RATIO_MAX': 0.25,         # Allow some caps\n",
    "        'EXCESSIVE_PUNCT_MIN': 4,       # Need many punctuation patterns\n",
    "        'UNIQUENESS_RATIO_MIN': 0.50,   # Allow some repetition\n",
    "        \n",
    "        # Tier 3: Advanced Features\n",
    "        'FLESCH_MAX': 90,               # Very simple text\n",
    "        'DALE_CHALL_MAX': 8.5,          # Very simple vocabulary\n",
    "        'BIGRAM_REPETITIVENESS_MIN': 0.95,  # Very high repetition\n",
    "        'COMMON_FAKE_NGRAMS_MIN': 4,    # Need many fake phrases\n",
    "        'TFIDF_SIMILARITY_MAX': 0.50    # Very generic text\n",
    "    },\n",
    "    {\n",
    "        'name': 'Balanced (Standard)',\n",
    "        # Tier 1: Essential Features\n",
    "        'SENTIMENT_EXTREME': 0.80,      # Moderate sentiment threshold\n",
    "        'WORD_COUNT_MIN': 15,           # Standard minimum length\n",
    "        'WORD_COUNT_MAX': 200,          # Standard maximum length\n",
    "        'ADJ_NOUN_RATIO': 2.5,          # Balanced adjective usage\n",
    "        'FIRST_PERSON_RATIO': 0.15,     # Moderate self-reference\n",
    "        \n",
    "        # Tier 2: Important Features\n",
    "        'SPAM_KEYWORD_MIN': 3,          # Standard spam detection\n",
    "        'CAPS_RATIO_MAX': 0.20,         # Standard caps threshold\n",
    "        'EXCESSIVE_PUNCT_MIN': 3,       # Standard punctuation\n",
    "        'UNIQUENESS_RATIO_MIN': 0.60,   # Standard repetition\n",
    "        \n",
    "        # Tier 3: Advanced Features\n",
    "        'FLESCH_MAX': 85,               # Moderate readability\n",
    "        'DALE_CHALL_MAX': 8.0,          # Moderate vocabulary\n",
    "        'BIGRAM_REPETITIVENESS_MIN': 0.90,  # Moderate repetition\n",
    "        'COMMON_FAKE_NGRAMS_MIN': 3,    # Standard fake phrases\n",
    "        'TFIDF_SIMILARITY_MAX': 0.45    # Moderate similarity\n",
    "    },\n",
    "    {\n",
    "        'name': 'Aggressive (High Recall)',\n",
    "        # Tier 1: Essential Features\n",
    "        'SENTIMENT_EXTREME': 0.70,      # Flag more sentiment extremes\n",
    "        'WORD_COUNT_MIN': 20,           # Flag more short reviews\n",
    "        'WORD_COUNT_MAX': 150,          # Flag longer reviews\n",
    "        'ADJ_NOUN_RATIO': 2.0,          # Flag more descriptive text\n",
    "        'FIRST_PERSON_RATIO': 0.12,     # Flag more self-reference\n",
    "        \n",
    "        # Tier 2: Important Features\n",
    "        'SPAM_KEYWORD_MIN': 2,          # Flag with fewer spam words\n",
    "        'CAPS_RATIO_MAX': 0.15,         # Flag more caps usage\n",
    "        'EXCESSIVE_PUNCT_MIN': 2,       # Flag more punctuation\n",
    "        'UNIQUENESS_RATIO_MIN': 0.65,   # Flag more repetition\n",
    "        \n",
    "        # Tier 3: Advanced Features\n",
    "        'FLESCH_MAX': 80,               # Flag simpler text\n",
    "        'DALE_CHALL_MAX': 7.5,          # Flag simpler vocabulary\n",
    "        'BIGRAM_REPETITIVENESS_MIN': 0.85,  # Flag more repetition\n",
    "        'COMMON_FAKE_NGRAMS_MIN': 2,    # Flag with fewer fake phrases\n",
    "        'TFIDF_SIMILARITY_MAX': 0.40    # Flag more generic text\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"Testing 3 data-tuned configurations:\")\n",
    "for config in threshold_configs:\n",
    "    print(f\"  - {config['name']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc708b4",
   "metadata": {},
   "source": [
    "## Cell 7: Test For the Best configuration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "42f1f4a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Testing: Conservative (High Precision)\n",
      "============================================================\n",
      "Accuracy:  52.30%\n",
      "Precision: 52.56%\n",
      "Recall:    47.20%\n",
      "F1 Score:  49.74%\n",
      "TP: 236, FP: 213\n",
      "TN: 287, FN: 264\n",
      "\n",
      "============================================================\n",
      "Testing: Balanced (Standard)\n",
      "============================================================\n",
      "Accuracy:  52.60%\n",
      "Precision: 52.47%\n",
      "Recall:    55.20%\n",
      "F1 Score:  53.80%\n",
      "TP: 276, FP: 250\n",
      "TN: 250, FN: 224\n",
      "\n",
      "============================================================\n",
      "Testing: Aggressive (High Recall)\n",
      "============================================================\n",
      "Accuracy:  52.50%\n",
      "Precision: 51.94%\n",
      "Recall:    67.00%\n",
      "F1 Score:  58.52%\n",
      "TP: 335, FP: 310\n",
      "TN: 190, FN: 165\n",
      "\n",
      "All configurations tested!\n"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "# CELL 7: Test All Configurations\n",
    "# ========================================\n",
    "all_results = []\n",
    "\n",
    "for config in threshold_configs:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Testing: {config['name']}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Create XAI engine with these thresholds\n",
    "    xai = FakeReviewXAI()\n",
    "    \n",
    "    # Update thresholds (copy config, remove 'name')\n",
    "    thresholds = {k: v for k, v in config.items() if k != 'name'}\n",
    "    xai.thresholds.update(thresholds)\n",
    "    \n",
    "    # Run tuning\n",
    "    results = tune_thresholds(validation_df, xai)\n",
    "    metrics = calculate_metrics(results)\n",
    "    \n",
    "    # Store\n",
    "    config_result = config.copy()\n",
    "    config_result.update(metrics)\n",
    "    all_results.append(config_result)\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"Accuracy:  {metrics['accuracy']:.2%}\")\n",
    "    print(f\"Precision: {metrics['precision']:.2%}\")\n",
    "    print(f\"Recall:    {metrics['recall']:.2%}\")\n",
    "    print(f\"F1 Score:  {metrics['f1_score']:.2%}\")\n",
    "    print(f\"TP: {metrics['true_positives']}, FP: {metrics['false_positives']}\")\n",
    "    print(f\"TN: {metrics['true_negatives']}, FN: {metrics['false_negatives']}\")\n",
    "\n",
    "print(\"\\nAll configurations tested!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cebfb59",
   "metadata": {},
   "source": [
    "## CELL 8: Save Results to threshold_tuning.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "e4dc282f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to: data/threshold_tuning.csv\n",
      "\n",
      "Results:\n",
      "                            name  accuracy  precision  recall  f1_score\n",
      "0  Conservative (High Precision)     0.523   0.525612   0.472  0.497366\n",
      "1            Balanced (Standard)     0.526   0.524715   0.552  0.538012\n",
      "2       Aggressive (High Recall)     0.525   0.519380   0.670  0.585153\n"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "# CELL 8: Save Results to CSV\n",
    "# ========================================\n",
    "results_df = pd.DataFrame(all_results)\n",
    "\n",
    "column_order = [\n",
    "    'name', 'SENTIMENT_EXTREME', 'WORD_COUNT_MIN', 'WORD_COUNT_MAX',\n",
    "    'ADJ_NOUN_RATIO', 'FIRST_PERSON_RATIO',\n",
    "    'CAPS_RATIO_MAX', 'EXCESSIVE_PUNCT_MIN', 'UNIQUENESS_RATIO_MIN',\n",
    "    'FLESCH_MAX', 'DALE_CHALL_MAX', 'BIGRAM_REPETITIVENESS_MIN',\n",
    "    'COMMON_FAKE_NGRAMS_MIN', 'TFIDF_SIMILARITY_MAX',\n",
    "    'accuracy', 'precision', 'recall', 'f1_score',\n",
    "    'true_positives', 'false_positives', 'true_negatives', 'false_negatives',\n",
    "    'total_samples'\n",
    "]\n",
    "\n",
    "results_df = results_df[column_order]\n",
    "results_df.to_csv('../data/threshold_tuning.csv', index=False)\n",
    "\n",
    "print(\"Results saved to: data/threshold_tuning.csv\")\n",
    "print(\"\\nResults:\")\n",
    "print(results_df[['name', 'accuracy', 'precision', 'recall', 'f1_score']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc56833e",
   "metadata": {},
   "source": [
    "## CELL 9: Display Best Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "2d71fe48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "BEST CONFIGURATION (By Accuracy)\n",
      "============================================================\n",
      "\n",
      "Name: Balanced (Standard)\n",
      "\n",
      "Performance:\n",
      "  Accuracy:  52.60%\n",
      "  Precision: 52.47%\n",
      "  Recall:    55.20%\n",
      "  F1 Score:  53.80%\n",
      "\n",
      "Thresholds:\n",
      "  SENTIMENT_EXTREME:      0.8\n",
      "  WORD_COUNT_MIN:         15\n",
      "  WORD_COUNT_MAX:         200\n",
      "  ADJ_NOUN_RATIO:         2.5\n",
      "  FIRST_PERSON_RATIO:     0.15\n",
      "  CAPS_RATIO_MAX:         0.2\n",
      "  EXCESSIVE_PUNCT_MIN:    3\n",
      "  UNIQUENESS_RATIO_MIN:   0.6\n",
      "  FLESCH_MAX:             85\n",
      "  DALE_CHALL_MAX:         8.0\n",
      "  BIGRAM_REPETITIVENESS_MIN: 0.9\n",
      "  COMMON_FAKE_NGRAMS_MIN: 3\n",
      "  TFIDF_SIMILARITY_MAX:   0.45\n"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "# CELL 9: Find Best Configuration\n",
    "# ========================================\n",
    "best_idx = results_df['accuracy'].idxmax()\n",
    "best_config = results_df.iloc[best_idx]\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"BEST CONFIGURATION (By Accuracy)\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nName: {best_config['name']}\")\n",
    "print(f\"\\nPerformance:\")\n",
    "print(f\"  Accuracy:  {best_config['accuracy']:.2%}\")\n",
    "print(f\"  Precision: {best_config['precision']:.2%}\")\n",
    "print(f\"  Recall:    {best_config['recall']:.2%}\")\n",
    "print(f\"  F1 Score:  {best_config['f1_score']:.2%}\")\n",
    "print(f\"\\nThresholds:\")\n",
    "print(f\"  SENTIMENT_EXTREME:      {best_config['SENTIMENT_EXTREME']}\")\n",
    "print(f\"  WORD_COUNT_MIN:         {best_config['WORD_COUNT_MIN']}\")\n",
    "print(f\"  WORD_COUNT_MAX:         {best_config['WORD_COUNT_MAX']}\")\n",
    "print(f\"  ADJ_NOUN_RATIO:         {best_config['ADJ_NOUN_RATIO']}\")\n",
    "\n",
    "print(f\"  FIRST_PERSON_RATIO:     {best_config['FIRST_PERSON_RATIO']}\")\n",
    "print(f\"  CAPS_RATIO_MAX:         {best_config['CAPS_RATIO_MAX']}\")\n",
    "print(f\"  EXCESSIVE_PUNCT_MIN:    {best_config['EXCESSIVE_PUNCT_MIN']}\")\n",
    "print(f\"  UNIQUENESS_RATIO_MIN:   {best_config['UNIQUENESS_RATIO_MIN']}\")\n",
    "\n",
    "print(f\"  FLESCH_MAX:             {best_config.get('FLESCH_MAX')}\")\n",
    "print(f\"  DALE_CHALL_MAX:         {best_config.get('DALE_CHALL_MAX')}\")\n",
    "print(f\"  BIGRAM_REPETITIVENESS_MIN: {best_config.get('BIGRAM_REPETITIVENESS_MIN')}\")\n",
    "print(f\"  COMMON_FAKE_NGRAMS_MIN: {best_config.get('COMMON_FAKE_NGRAMS_MIN')}\")\n",
    "print(f\"  TFIDF_SIMILARITY_MAX:   {best_config.get('TFIDF_SIMILARITY_MAX')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b3e15b",
   "metadata": {},
   "source": [
    "## CELL 11: Generate config.py Code\n",
    "updates config.py with optimal thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "2971eb59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config.py successfully updated!\n",
      "\n",
      "File location: c:\\Users\\Amr\\Documents\\GitHub\\Team-C-repository\\xai_engine\\config.py\n"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "# CELL 11: Auto-Update config.py\n",
    "# ========================================\n",
    "import os\n",
    "\n",
    "\n",
    "# Generate the new config content\n",
    "new_config_content = f'''\"\"\"\n",
    "XAI Engine Configuration\n",
    "Centralized thresholds for all features (Tier 1 + Tier 2 + Tier 3)\n",
    "Auto-generated by threshold_tuning.ipynb\n",
    "\"\"\"\n",
    "\n",
    "# ============================================================================\n",
    "# ALL THRESHOLDS (TIER 1 + TIER 2 + TIER 3) - OPTIMIZED\n",
    "# ============================================================================\n",
    "\n",
    "THRESHOLDS = {{\n",
    "    # TIER 1: Essential Features\n",
    "    'SENTIMENT_EXTREME': {best_config['SENTIMENT_EXTREME']},\n",
    "    'WORD_COUNT_MIN': {int(best_config['WORD_COUNT_MIN'])},\n",
    "    'WORD_COUNT_MAX': {int(best_config['WORD_COUNT_MAX'])},\n",
    "    'ADJ_NOUN_RATIO': {best_config['ADJ_NOUN_RATIO']},\n",
    "    'FIRST_PERSON_RATIO': {best_config['FIRST_PERSON_RATIO']},\n",
    "    \n",
    "    # TIER 2: Important Features\n",
    "    'CAPS_RATIO_MAX': {best_config['CAPS_RATIO_MAX']},\n",
    "    'EXCESSIVE_PUNCT_MIN': {int(best_config['EXCESSIVE_PUNCT_MIN'])},\n",
    "    'UNIQUENESS_RATIO_MIN': {best_config['UNIQUENESS_RATIO_MIN']},\n",
    "    \n",
    "    # TIER 3: Advanced Features\n",
    "    'FLESCH_MAX': {best_config['FLESCH_MAX']},  # Flag if readability too high (fake-like simplicity)\n",
    "    'DALE_CHALL_MAX':{best_config['DALE_CHALL_MAX']},  # Flag if too simple vocabulary\n",
    "    'BIGRAM_REPETITIVENESS_MIN': {best_config['BIGRAM_REPETITIVENESS_MIN']},  # Flag if too repetitive\n",
    "    'COMMON_FAKE_NGRAMS_MIN': {best_config['COMMON_FAKE_NGRAMS_MIN']},  # Flag if >=2 common fake phrases\n",
    "    'TFIDF_SIMILARITY_MAX': {best_config['TFIDF_SIMILARITY_MAX']}  # Flag if too similar to generic reviews\n",
    "}}\n",
    "\n",
    "\n",
    "def load_config():\n",
    "    \"\"\"Return a copy of thresholds\"\"\"\n",
    "    return THRESHOLDS.copy()\n",
    "\n",
    "\n",
    "def update_thresholds(new_thresholds):\n",
    "    \"\"\"Update thresholds (used by threshold tuning)\"\"\"\n",
    "    THRESHOLDS.update(new_thresholds)\n",
    "'''\n",
    "\n",
    "# Write to config.py\n",
    "config_path = '../xai_engine/config.py'\n",
    "\n",
    "try:\n",
    "    with open(config_path, 'w') as f:\n",
    "        f.write(new_config_content)\n",
    "    \n",
    "    print(\"config.py successfully updated!\")\n",
    "    print(f\"\\nFile location: {os.path.abspath(config_path)}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error updating config.py: {e}\")\n",
    "    print(\"\\nManual update required. Copy this code:\\n\")\n",
    "    print(new_config_content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
